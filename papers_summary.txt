
================================================================================
PROCESSING: 2305.14325_Multiagent_Debate.pdf
================================================================================

Improving Factuality and Reasoning in Language
Models through Multiagent Debate
YilunDu ShuangLi AntonioTorralba JoshuaB.Tenenbaum
MITCSAIL MITCSAIL MITCSAIL MITCSAIL,BCS,CBMM
yilundu@mit.edu lishuang@mit.edu torralba@mit.edu jbt@mit.edu
IgorMordatch
GoogleBrain
imordatch@google.com
Abstract
Large language models (LLMs) have demonstrated remarkable capabilities in
language generation, understanding, and few-shot learning in recent years. An
extensivebodyofworkhasexploredhowtheirperformancemaybefurtherim-
provedthroughthetoolsofprompting,rangingfromverification,self-consistency,
orintermediatescratchpads. Inthispaper,wepresentacomplementaryapproach
toimprovelanguageresponseswheremultiplelanguagemodelinstancespropose
anddebatetheirindividualresponsesandreasoningprocessesovermultiplerounds
to arrive at a common final answer. Our findings indicate that this approach
significantlyenhancesmathematicalandstrategicreasoningacrossanumberof
tasks. We also demonstrate that our approach improves the factual validity of
generated content, reducing fallacious answers and hallucinations that contem-
porary models are prone to. Our approach may be directly applied to existing
black-boxmodelsandusesidenticalprocedureandpromptsforalltasksweinves-
tigate. Overall,ourfindingssuggestthatsuch"societyofminds"approachhasthe
potentialtosignificantlyadvancethecapabilitiesofLLMsandpavethewayfor
furtherbreakthroughsinlanguagegenerationandunderstanding. Projectwebsite
athttps://composable-models.github.io/llm_debate/.
1 Introduction
Largelanguagemodels(LLMs)havedemonstratedremarkablelanguagegeneration,understanding,
andfew-shotlearningcapabilitiesinrecentyears. Thesemethodsaretrainedonamassivecorpusof
textontheinternet,wherethequalityandaccuracyofextractednaturallanguagemaynotbeensured.
Thus,currentmodelsmaysufferfromconfidentlyhallucinatingfactsormakingimplausiblejumpsin
chainsofreasoning. Anextensivebodyofrecentworkhasfocusedonimprovingfactualaccuracy
and reasoning in language models. These range from prompting models with few or zero-shot
chain-of-thoughtdemonstrations,useofverification,self-consistency,orintermediatescratchpads.
We note that these techniques are applied over a single model instance. Instead, we propose a
complementary approach inspired by The Society of Mind [19] and multi-agent settings, where
multiplelanguagemodelinstances(oragents)individuallyproposeandjointlydebatetheirresponses
and reasoning processes to arrive at a single common answer. More specifically, given a query,
multipleinstancesofalanguagemodelfirstgenerateindividualcandidateanswerstoaquery. Then
eachindividualmodelinstancereadsandcritiquestheresponsesofallothermodelsandusesthis
contenttoupdateitsownanswer. Thisstepisthenrepeatedoverseveralrounds. Thisprocessinduces
modelstoconstructanswersthatareconsistentwithboththeirinternalcriticaswellassensiblein
Preprint.Underreview.
3202
yaM
32
]LC.sc[
1v52341.5032:viXra
ycaruccA
Single Model Multi-Model Debate
100
75
50
25
0
Biographies MMLU Chess Move Arithmetic Grade School Chess Move
Validity Math Optimality
Figure1:MultiagentDebateImprovesReasoningandFactualAccuracy.Accuracyoftraditionalinference
andourmulti-agentdebateoversixbenchmarks(chessmoveoptimalityreportedasanormalizedscore)
lightoftheresponsesofotheragents. Theresultingquorumofmodelscanholdandmaintainmultiple
chainsofreasoningandpossibleanswerssimultaneouslybeforeproposingthefinalanswer.
We find that our debate approach outperforms single model baselines such as zero-shot chain of
thought [11]andreflection [26,18]onavarietyofsixreasoning,factuality,andquestion-answering
tasks. Using both multiple model agents and multiple rounds of debate are important to achieve
the best performance. Given an initial query, we find that individual model instances propose a
diverserangeofanswersdespitebeingthesamemodelclass(althoughwealsoinvestigatethecaseof
mixingdifferentmodeltypes,suchaschatGPT [21]andBard [23]). Afterdebatingandexamining
theresponsesofothermodelinstances,wefindthatthepopulationalmostalwaysconvergesona
singleandmoreaccuratecommonanswer. Debateresultsarealsolesslikelytoincludefalsefacts
thatmodelsareinternallyuncertainof. Thisisbecauseasthedebateprogresses,individualmodel
instancestendtodisagreeonuncertainfactsandomitthemfromtheanswer(Figure7). Lastly,we
findthatdebatedoesnotjustacttoamplifyonecorrectanswerinamodelquorum-wefindmany
caseswhereallthemodelsinitiallymakeincorrectpredictions,butthenarriveatthecorrectanswer
asdebateprogresses(Figure4,11).
Weusethesamemethodologyandprompttemplatesforallourtasksandrequireonlyblack-box
accesstolanguagemodelgenerations‚Äìnomodel-internalinformationsuchaslikelihoodsorgradients
isneeded. Thisallowsourmethodtobeusedwithcommonpublicmodelsservinginterfaces. The
method is also orthogonal to other model generation improvements such as retrieval or prompt
engineering(infact,wecombineourdebatemethodwithzero-shotchainofthought). Whilethe
debateprocessismorecostly,requiringmultiplemodelinstancesandrounds,itarrivesatsignificantly
improvedanswersandmaybeusedtogenerateadditionalmodeltrainingdata,effectivelycreatinga
modelself-improvementloop.
To help evaluate the effect of our approach on factual accuracy, we introduce a new benchmark
and dataset evaluating factual accuracy of famous computer scientist biographies. We find that
contemporarylanguagemodelshaveanespeciallyhightendencytohallucinatefactuallyincorrect
biographies,oftenmisrepresentingtherelevantinstitutionsanddates. Moreover,thesefactsoften
inconsistentacrossdifferentlanguagemodelinstances. Byaskingmodelstocometoaconsensus
acrosstheiranswers,suchinconsistentfactsmaybeeitherremovedorcorrected.
Insummary,ourworkcontributesthefollowing. First,wepresentanovelapproachtoimproving
factualcorrectnessandreasoningaccuracyincontemporarylanguagemodels,leveragingamulti-
agentdebateprocessbetweenmodels. Second,weintroduceanewbenchmarkoffactualcorrectness
whichcontemporarylanguagemodelsstrugglewith. Finally,weevaluatetheperformanceofour
debate procedure in language generation, both in terms of the number of agents, the underlying
roundsofdebate,andthepromptsthatelicitsuchbehavioracrossasetofsixdifferentreasoningand
factualaccuracytasks.
2 LanguageGenerationthroughMultiagentDebate
Wepresentanapproachtogeneratelanguageresponsesthroughmultiagentdebate. Weprovidean
overviewofourapproachinSection2.1. Wefurtherdiscussconvergencetoconsensusinthedebate
processinSection2.2. TheoveralloverviewofourapproachisshowninFigure2.
2
User Input: A treasure hunter found a buried treasure chest filled with gems. There were 175 diamonds, 35 fewer rubies than
diamonds, and twice the number of emeralds than the rubies. How many of the gems were there in the chest?
Round 1
Agent 1: Let x be the number of rubies. Then the number of diamonds is 175. Agent 2: If there were 175 diamonds,
And the number of emeralds is 2(x-35), since there are twice as many then the number of rubies is 35 fewer,
emeralds as rubies and there are 35 fewer rubies than diamonds. or 175 -35 = 140 rubies.
To find the total number of gems, we add the number of diamonds, rubies, And since there were twice as many
and emeralds: Total gems = 175 + x + 2(x-35) = 3x + 105. emeralds as rubies, we can represent the
number of emeralds as 2 * 140 = 280
We don't know the exact value of x, but we do know that it represents the emeralds.
number of rubies, which is less than 175 (since there are 35 fewer rubies than
diamonds). Let's try some values of x: If x = 50, then the total gems = 3(50) + To find the total number of gems in the
105 = 255. If x = 100, then the total gems = 3(100) + 105 = 405. chest, we can add up the number of
diamonds, rubies, and emeralds:
Since we know there were fewer rubies than diamonds, the answer must be 175 + 140 + 280 = 595. Therefore, there
closer to 255 than to 405. After some trial and error, we can find that x = 40 were 595 gems in the chest.
gives us: Total gems = 3(40) + 105 = 225. ‚úó ‚úì
User Input: Using the solutions from other agents as additional information, can you give an updated response....
(Copy Agent 2‚Äôs reply) (Copy Agent 1‚Äôs reply)
Round 2
Agent 1: Given the information provided in other agents' solutions, we have Agent 2: After reviewing the solutions
two answers: One agent found that the number of rubies is 40 and the total provided by other agents, I agree with
number of gems is 225. Another agent found that the number of rubies is the second agent who found that the
140 and the total number of gems is 595. number of rubies is 140 and the total
number of gems in the chest is 595.
We know that the number of rubies is 35 fewer than the number of Therefore, my answer to the math
diamonds, which is 175. Sothe number of rubies should be 175 -35 = 140, problem is 595.
as the second agent found. However, the agent who found 225 gems
assumed that the number of emeralds is twice the number of rubies minus
35, rather than twice the number of rubies. If we use this assumption, we
get: Number of rubies = 140. Number of emeralds = 2 * 140 -35 = 245.
‚úó ‚úì
Total number of gems = 175 + 140 + 245 = 560. Our final answer is 560.
User Input: Using the solutions from other agents as additional information, can you give an updated response....
(Copy Agent 2‚Äôs reply) (Copy Agent 1‚Äôs reply)
Round 3
Agent 2: After reviewing the solutions
Agent 1: After reviewing the solutions provided by other agents, I agree
provided by other agents, I also agree
with the second agent who found that the number of rubies is 140 and the
that the number of rubies is 140 and the
total number of gems in the chest is 595. Therefore, my answer to the math
total number of gems in the chest is 595.
problem is 595.
Therefore, my answer to the math
‚úì ‚úì
problem is also 595.
Figure2:IllustrationofDebate.Illustrationofthedebateprocedure.
2.1 MultiagentLanguageGeneration
Consideryourworkprocesswhensolvingthefollowingmathquestiononanexam: ‚ÄúWhatisthe
areaofatrianglewithsidelengthsof3,4,5?". Inonethreadofwork,youmayrecognizethatthe
triangleside-lengthsdirectlycorrespondtoarighttriangle,andthusdirectlycomputetheareaas
0.5√ó3√ó4=64. Tomakesurethatyouhavetherightanswer,youmaythentrytosolvetheproblem
differentlybyestimatinganangleŒ∏inthetriangleusingtheLawofCosines,andthenobtainthearea
byusingtheformula0.5√ó3√ó4√ósin(Œ∏),arrivingatanotheranswertothegivenexamproblem.
When these lines of work give the same answer, your confidence about the answer increases. In
contrast,whentheseanswersaredifferent,individuallinesofworkmayengageinamental‚Äúdebate"
procedure,whereyoucloselycross-examinethereasoningandassumptionsofeachlineofworkand
refinesolutionsuntilaconsistentanswer.
Similarly,considerwritingabiographyofahistoricalfigure. Toensurethefactualityofthebiography,
youmayconsultmultipledifferentsourcesoneachfact. Factsthatareconsistentineachsource
increase your confidence about the fact. In contrast, facts that are inconsistent require careful
cross-examinationbetweensourcestodeterminethefinalconsistentdata.
3
DebateLength Prompt
"Thesearethesolutionstotheproblemfromotheragents:[otheranswers]
Short
Basedofftheopinionofotheragents,canyougiveanupdatedresponse..."
"Thesearethesolutionstotheproblemfromotheragents:[otheranswers]
Long
Usingtheopinionofotheragentsasadditionaladvice,canyougiveanupdatedresponse..."
Figure3: Promptstoinducelongandshortformdebate. Responsesofotheragentstoquestionsareare
insertedinthemiddleoftheprompt(indicatedwith[otheranswers])
Tomimictheabovemulti-threadedreasoningprocessandmulti-sourcefactualitycheckingprocesses,
weproposetogenerateanswerssubjecttoamulti-agentdebateprocedurebetweenmultipleinstances
oflargelanguagemodels. Givenaquestion,multipleagentsrepresentedascopiesofalargelanguage
model,generateanswerstothequestion. Eachresponseservesasapossiblethoughtprocessorsource
ofinformationwhichagentsmayre-examinetofindconsistentfinalanswers.
Afterinitialresponsesaregeneratedfromdifferentagents,weinitiatearoundofdebatebetween
agents. Individualresponsesfromotheragentsareconcatenatedandgivenascontexttoeachagent,
witheachagentinstructedtoconstructanewresponsebasedonsuchresponses. Eachlanguageagent
isthusresponsibleforbothverifyingthecollectionofresponsesgivenbyotheragents,andrefining
itsownresponsebasedonotheragents‚Äôresponses. Weiterativelyrepeatthisdebateprocedureover
multipleroundsforimprovedperformance.
Concretely, we first prompt each agent to independently solve the given problem or task. After
each agent generatesa response, we feedeach agent a consensus prompt, illustrated in Figure 3,
whereeachagentisinstructedtoupdatetheirresponsesbasedontheresponsesofotheragents. This
resultantconsensuspromptmaythenberepeatedlygiven,usingtheupdatedresponsesofeachagent.
WeillustrateanoverviewofthismultiagentdebateprocedureinFigure2.
Notethatourproposedapproachoperatesinanorthogonalmannertoexistingapproachestoprompt
languagemodels. Givenaquestion,wemayapplyadditionaltechniquesforpromptinglanguage
models to further improve our debate procedure by eliciting additional more detailed responses
from language models. We illustrate the synergy of our approach with existing approaches to
promptinglanguagemodelsinFigure6anddirectlyapplyzero-shotchain-of-thoughtreasoningin
ourevaluations.
2.2 ConsensusinDebates
Givenmultipleroundsofdebate,howcanweensurethatasetoflanguagemodelagentswillconverge
toafinalconsensusanswer? Ingeneral,debatecanbeseenasamulti-agentgame,whereconvergence
isnotguaranteed. Empirically, however, wefindthatlanguagemodelsareabletoconvergeona
singlesharedansweraftermultipleroundsofdebate(Figure4).
Wefoundthatwecouldcontrolthedurationofdebatesbyhowchanginghowmuchalanguagemodel
trustsitsownoutputsoverthosegeneratedbyothermodelsthroughdifferentprompts. Weillustrate
twopromptsbelowinFigure3,whichweusetoinducedifferentdebatedurationsbetweenlanguage
models,andillustratetheeffectofsuchpromptsinFigure12. Ingeneral,wefoundthatpromptsthat
encouragedmodelstobemore‚Äústubborn‚Äôbasedontheirownsolutionsledtolongerdebatesand
betterfinalsolutions. Overall,weobservedthatlanguagemodelagentswererelatively"agreeable",
perhapsasaresultofinstructiontuningorreinforcementlearningbasedonhumanfeedback [22].
3 Experiments
Inourexperiments,weevaluateourmultiagentdebateprocedureandanswerthefollowingquestions:
(1)Towhatextentdoesmultiagentdebateimprovereasoning? (2)Towhatextentdoesmultiagent
debate improve factual validity? (3) What design choices enable multiagent debate to improve
languagegenerationperformance?
3.1 ImprovingReasoningwithMultiagentDebate
Wefirstevaluatetheextenttowhichmultiagentdebateimprovestheunderlyingreasoningprocessin
languagemodels.
Tasks. Weevaluateourapproachonthreereasoningtasksofincreasingdifficulty:
4
Question: What is the result of 10+20*23+3-11*18? Question: What is the result of 3+7*9+19-21*18?
Round 1 Agent 1: 269 ‚úó Agent 2: 369 ‚úó Agent 1: 378 ‚úó Agent 2: -351 ‚úó Agent 3: -357 ‚úó
Round 2 Agent 1: 275 ‚úì Agent 2: 275 ‚úì Agent 1: -293 ‚úì Agent 2: -293 ‚úì Agent 3: 19 ‚úó
Question: What is the result of 4+23*6+24-24*12? Question: What is the result of 8+14*15+20-3*26?
Round 1 Agent 1: -244 ‚úó Agent 2: -146 ‚úó Agent 1: 236 ‚úó Agent 2: -214 ‚úó Agent 3: 210 ‚úó
Round 2 Agent 1: -146 ‚úó Agent 2: -122 ‚úì Agent 1: 160 ‚úì Agent 2: 160 ‚úì Agent 3: 160 ‚úì
Round 3 Agent 1: -122 ‚úì Agent 2: -122 ‚úì Agent 1: 160 ‚úì Agent 2: 160 ‚úì Agent 3: 160 ‚úì
Figure4:IllustrationofSolvingMath.Reasoningbetweenagentsisomitted.
Question: Regina wrote 9 novels last year. If Question: Dennis uses 1 pound of butter for every dozen
this is 3 quarters of the number of novelsshe croissants that he makes. He needs to make 6 dozen croissants.
has written this year, how many novels has she The grocery store currently has a promotion for buy one pound
of butter get one half off. If the butter costs $4.00 a pound,
written this year?
how much will it cost him to purchase 6 pounds of butter?
Round 1 Agent 1: 48 ‚úó Agent 2: 12 ‚úì Agent 1: 18 ‚úì Agent 2: 30 ‚úó
Round 2 Agent 1: 12 ‚úì Agent 2: 12 ‚úì Agent 1: 18 ‚úì Agent 2: 18 ‚úì
Figure5:IllustrationofSolvingGradeSchoolMath.Reasoningbetweenagentsomitted.
‚Ä¢ Arithmetic. Wefirstevaluatetheabilityofmodelstocorrectlyevaluateanarithmeticexpression
(containingaddition,multiplication,andsubtraction)consistingofsixdifferenttwo-digitnumbers.
Forexample: Whatistheresultof12+15*21+0-3*27?
‚Ä¢ GSM8K.Next,weconsiderhardermathematicalreasoningtasks. UsingtheGSM8Kdataset[3],
themodelsmustcorrectlysolvegradeschoolmathematicalreasoningtasks.
‚Ä¢ Chess Move Prediction. Finally,weconsiderthestrategicreasoningoftheabilityofmodels,
andaskmodelstopredictthebestnextmoveinagameofchess,giventhefirst14movesofachess
gamebetweentwochessgrand-mastersdescribedinPGNnotation[6].
WereporttheaccuracyoffinalanswersinarithmeticandGSM8Ktasksandreportthepawnscore
(advantage) of predicted moves, as estimated by Stockfish in the Chess move prediction tasks.
AdditionaldetailsmaybefoundintheAppendix.
ycaruccA
ksaT
K8MSG
Baselines. Wecompareourapproachtothreealter-
Single Agent Multi-Agent Debate
nativeapproachestogenerateresponsesforreasoning
90%
problems. First, we ask agents to directly generate
responses(singleagent). Next,weconsideraskinglan- 85%
guagemodelstogenerateandthen"self-reflect"onthe
80%
responsesgenerated[26,18]. Finally,weconsidergen-
eratingresponsesusingmultipleagentsandperforming 75%
majority voting [15, 3]. As the focus of our experi-
70%
mentsistoverifytheeffectivenessofmultiagentagent No Chain of With Chain of
debate,werunbothbaselinesandourapproach,using Thought Thought
theidenticalstartingpromptandlanguagemodelacross Figure6: SynergywithOtherMethods. Per-
allevaluations. Weevaluatemodelsinazero-shotset- formanceofdebateincreaseswithuseofChain
ting,withpromptsfoundintheAppendixofthepaper. ofThoughtprompting.
WeusechatGPT-basedlanguagemodel [21]inallourexperimentsexceptthoseinFigure11where
wecomparemultiplelanguagemodels.
Due to computational expense, we evaluate our approach across benchmarks mainly using three
agentswithtworoundsofdebates,althoughwefoundfurthergainswithbothmoreagentsandrounds
ofdebate(Figure10). AdditionalevaluationdetailsarefoundintheAppendix.
QuantitativeResults. InTable1,wereporttheresultsofeachapproachonarithmetic,gradeschool
math,andchessreasoningtask. Ineachtask,weobservethatutilizingmultipledifferentagentsto
generatesolutionsimprovesperformanceoverusingasinglelanguagemodelagenttogeneratea
solution. Simultaneously,wealsoseethatreflection,wherealanguagemodelisaskedtocritique
itsearlygeneration,generallygivesamodestboostinperformance. Multiagentdebate,whichmay
beseenasacombinationofbothreflectionandmultiagentgeneration,givesasubstantialboostin
reasoningacrosseachofthetasks.
5

================================================================================
PROCESSING: 2306.00622_ReviewerGPT.pdf
================================================================================

ReviewerGPT? An Exploratory Study on Using
Large Language Models for Paper Reviewing
Ryan Liu and Nihar Shah
{ryanliu, nihars}@andrew.cmu.edu
Carnegie Mellon University
Abstract
Giventherapidascentoflargelanguagemodels(LLMs),westudythequestion: (How)canlargelan-
guagemodelshelpinthereviewingofscientificpapersorproposals? Wefirstconductsomepilotstudies
where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly,
OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outper-
forms prompting to simply write a review. With these insights, we study the use of LLMs (specifically,
GPT-4) for three tasks:
1. Identifying errors: We construct 13 short computer science papers each with a deliberately
inserted error, and ask the LLM to check for the correctness of these papers. We observe that the
LLM finds errors in 7 of them, spanning both mathematical and conceptual errors.
2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the
respectivesectionsof15NeurIPS2022papers. Wefindthatacross119{checklistquestion,paper}
pairs, the LLM had an 86.6% accuracy.
3. Choosing the ‚Äúbetter‚Äù paper: We generate 10 pairs of abstracts, deliberately designing each
pair in such a way that one abstract was clearly superior than the other. The LLM, however,
struggled to discern these relatively straightforward distinctions accurately, committing errors in
its evaluations for 6 out of the 10 pairs.
Basedontheseexperiments,wethinkthatLLMshaveapromisinguseasreviewingassistantsforspecific
reviewing tasks, but not (yet) for complete evaluations of papers or proposals.
1 Introduction
Large language models (LLMs) have recently been found to excel in many different domains. Their success
ingeneralpurposetasksalsoraisesthenaturalquestion: cantheybeusedforreviewingscientificpapers(or
proposals)? Peer review is highly strained due to fast increasing numbers of submissions and overburdening
of reviewers [McC06; Sha22]. It is estimated that millions of hours of researchers‚Äô time is spent in review-
ing [The13]. Furthermore, several controlled experiments have found that flawed papers frequently get past
humanreviewers[Bax+98;GGM98;Sch+04;Sch+08]. LLMsholdaconsiderablepotentialinrelievingsome
of these issues in the scientific review process.
With this motivation, we conduct an exploratory study on whether and how LLMs can be used for
reviewing. We first conduct a pilot (Appendix A) to select the model and prompting strategies. In the
pilot, we first compared various models (GPT-4 [Ope23b], Bard [Man23], Vicuna [The23], Koala [Gen+23],
Alpaca [Tao+23], LLaMa [Tou+23], Dolly [Con+23], OpenAssistant [LAI23], and StableLM [Sta23]) by
asking them to find errors in a short paper. We found that GPT-4 was the only model that was successful
at this task. We also piloted prompting strategies, and found that by asking the model targeted questions,
the responses generated are significantly more useful compared to requesting it to simply ‚Äòwrite a review‚Äô.
With these choices in place, we then evaluated the following three reviewing tasks on GPT-4, employing
targeted prompts for each task:
1
3202
nuJ
1
]LC.sc[
1v22600.6032:viXra
Can LLMs identify errors in papers? (Section 3) A primary objective of the scientific reviewing
process is to identify any shortcomings in scientific manuscripts and ensure that only robust and accurate
scientific content gets published. To evaluate the LLM‚Äôs efficacy in achieving this objective, we constructed
13shortpapers,intentionallyinfusingeachofthemwithakeyerror. Theerrorsspannedfrommathematical
errors (e.g., wrong implication of mean squared convergence) to logical fallacies (e.g., false inference drawn
from a data analysis). The LLM identified the errors in 7 of the 13 short papers.1 This rate is comparable
to that of human peer reviewers [Bax+98; GGM98; Sch+04; Sch+08].
Can LLMs verify author-provided checklists? (Section 4)Manyconferencesandjournalsprovide
authorswithachecklistofitemsthathelpssetexpectationsfrompapersandalsooffersaguidelinetoauthors
to meet these expectations. In venues such as NeurIPS,2 the authors need to submit their responses to the
checklist along with the submission. The checklist items are verified by reviewers, and failing to properly
adhere to the checklist can get the paper rejected. We investigate if LLMs can do this job of verifying
whether the checklist items are accurately reported by the authors. In a set of 15 papers from NeurIPS
2023, we selected checklist items where the authors answered ‚ÄúYes‚Äù, manually labeled their ground truth
answers, and prompted the LLM to answer the same checklist items by providing the relevant section(s) of
thepaper. Wefoundthatacross119unique{checklistquestion,paper}pairs,(i)takingthemajorityanswer
over 3 responses, the LLM achieved 86.6% accuracy compared to the ground truth label; (ii) coincidentally,
86.6% of author responses matched our ground truth label; (iii) LLM answers disagreed with 75% of mis-
matched author responses, and (iv) 50% of the LLM‚Äôs errors were due to questions where the answers were
undeterminable with the text in the paper (e.g., requiring information provided in figures).
Can LLMs select the better of two papers? (Section 5)WiththesuccessfulperformanceofLLMs
on the aforementioned two specific tasks, we moved on to a more ambitious question of whether LLMs can
select the better of two papers in a selective venue3 or the better of two proposals in a grant evaluation
setting. To this end, we considered arguably the simplest of tasks: For a given pair of abstracts where one
abstractisconstructedtobesuperiortoanother(e.g.,bothabstractsareidenticalbutonehasanadditional
result), can the LLM identify the better abstract? We constructed 10 such pairs of abstracts. In a subset
of these pairs, we also inserted certain distractions such as the use of bombastic language or buzzwords
in the inferior abstract. We then tested the LLM‚Äôs ability to accurately identify the superior abstract in
these scenarios. We found that the LLM performs surprisingly poorly at this task, erring in 6 of the 10
cases. The four cases where it succeeded involved identifying an abstract‚Äôs incorrect interpretation of a null
result, accurately interpreting upper bounds, remaining unaffected by buzzwords, and disregarding author
identities. On the other hand, the six failures encompassed a bias towards positive results, misinterpreting
parameter ranges, misinterpreting lower bounds, falling victim to a prompt injection attack, being swayed
by bombastic language in the inferior abstract, and getting influenced by the name of the algorithm.
Overall,ourresultssuggestthatLLMs(particularlyGPT-4atitscurrentstage)haveconsiderablepromise
in being used for specific parts of the review process, but are not yet capable of functioning as a standalone
reviewer.
2 Related work
In this section we discuss literature that is most closely related to our work.
Large language models. Recent advances in LLMs have spurred the investigation of the general capa-
bilities of these models [Bub+23; Ope23b; Ani+23]. On one hand, natural language processing (NLP)
researchers have designed methods to address and improve the factuality and reasoning capabilities of
1While GPT-4 identified the errors in 7 of the 13 papers, we also subsequently tried other models‚ÄîBard, Vicuna, Koala,
Alpaca,LLaMa,Dolly,OpenAssistant,andStableLM.Noneofthesemodelscouldidentifyanyerrorinanyofthe13papers.
2ShortforNeuralInformationProcessingSystems‚Äìatop-tiermachinelearningconference.
3Thereareevidence-basedarguments[Ras+22]challengingthenotionofpapersbeingdirectlycomparable,renderingques-
tionsabout‚Äúbetter‚Äùpapersseeminglyfutile. Nevertheless,wedelveintothisinquiryduetothepersistentpursuitofselectivity
inmostpeer-reviewvenues. Additionally,selectivitybecomescrucialinvariousothercontexts,suchastheevaluationofgrant
proposals,wherelimitedresourcesnecessitateselectivedistributionamongresearchers.
2
LLMs [Du+23; Wei+22; Xia+22; ZJH23]. On the other hand, investigations on the capabilites of LLMs
have spread from NLP to various domains, such as agent simulation, data generation, computational social
science, human-computer interaction, and health/law.
Under simulation, LLM generative capabilities have been used for embodied agent planning [Son+22;
Wan+23] and human agent simulation [Par+23]. Data generation has seen advances in either simplifying or
fully automating the creation of questions and labels [Par+22; Liu+22; SS21; Wan+21]. In computational
social science, there have been general studies [Zie+23] as well as targeted investigations within areas such
as theory of mind [Ull23] and moral judgment [Jin+22]. Human-computer interaction has explored using
LLMs to generate believable accounts of HCI experiences [HTK23]. Finally, LLMs have also been used in
health and biomedical applications [Liu+23; Che+23a; Che+23b] and legal processes [Nay23].
Automation in peer review. Thepeerreviewprocessinmanyvenuesalreadyhasaconsiderableamount
ofautomationdeployed. Thepartofthereviewprocessthatinvolvesthemostautomationisthatofassigning
suitablereviewerstopapers. Anumberofalgorithmshavebeenusedtocomputetheexpertiseofreviewersfor
papers[CZ13;Wie+19;Coh+20],andvariousalgorithmshavealsobeenusedtoassignreviewerstopapersin
a manner that ensures a high expertise of the assigned reviewers [CZ13; SSS21b; KSM19]. Following recent
discoveriesof‚Äúcollusionrings‚Äù[Vij20; Lit21]inpeerreview,severalvenueshaveincorporatedrandomization
into their assignment algorithms [Jec+20] to mitigate this issue.
Anumberofvenueshaveusedalgorithms[NSP21]toaddresstheissueofsubjectivityorcommensuration
bias[Lee15]. Somevenueshavealsousedalgorithmstoaddresstheissueofmiscalibration[GWG13],however,
these calibration algorithms have not worked well [Sha22, Section 5]. Outside of computer science, there
are algorithms designed specifically for checking that the submitted papers follow suitable submission and
reportingguidelines[Hou16;FMG19]. Variousothergeneralcomputationaltoolsareusedinthepeer-review
processes, e.g., plagiarism checkers, scripts to detect conflicts of interest based on databases like DBLP, etc.
Some previous attempts at designing machine learning algorithms to conduct comprehensive paper reviews
have shown limited success in generating appropriate reviews, but these algorithms have proven useful in
other capacities such as generating paper summaries [Hua18; Wan+20; YLN21].
Therearealsonascentinvestigationsintousinglanguagemodelsforpeerreviewandadjacentareas. The
paper[HH23]summarizesthedesiredrolesofagentsandobjectivesinpeerreview,andproviderecommenda-
tionsforthesafeuseofLLMsinthiscontext. [Zha+22]useRoBERTa[Liu+19]toassessthetextualfluency
of papers and sentiment of reviews in an analysis of peer review fairness. And in peer grading, [Mor+23]
use a subset of rater data in massively open online courses to finetune distilBERT [San+20], and use it to
validate peer grading scores.
Peer-review datasets. This paper also contributes a small-scale dataset for evaluating machine learning
/ language models for reviewing papers. Any LLMs subsequently designed may be evaluated on the 13
short papers and the pilot short paper we have constructed with inserted ‚Äúgold standard‚Äù flaws, where we
currently find that only GPT-4 has any success in detecting errors. Similarly, LLMs may be evaluated on
the 119 {checklist question, paper} pairs we have labeled. On the 10 pairs of abstracts we have constructed,
their accuracy on direct comparative evaluations and their resistance to distractions can be examined. Of
course, this assumes that the LLM is not itself trained on this very paper. In addition, our aforementioned
constructions can also be used for few-shot learning or prompting of these models.
We now review other useful datasets pertaining to (peer) reviewing. Given the fact that the assignment
of reviewers to papers is the most automated part of peer review, the paper [Ste+23] releases a ‚Äúgold
standard‚Äù dataset of expertise of reviewers for papers to train and/or evaluate such algorithms. Motivated
by the absence of any datasets for the problem of mitigating collusion rings, the paper [Jec+23] releases
a dataset on collusion strategies constructed from a mock peer-review exercise. Another issue is strategic
behavior of reviewers where they manipulate reviews they are providing (e.g., lower scores of papers they
are reviewing) to improve the chances of their own paper, and the paper [SSS21a] releases a dataset of such
strategies also obtained from a mock peer-review exercise. The paper [Ker+20] releases anonymized review
ratings and author-provided feedback.
3
In the past few years, platforms such as OpenReview.net and conferences such as ICLR and NeurIPS
release the peer reviews along with the papers. Such reviews have been compiled and released as useful
datasets[Kan+18]. Thereleaseofreviewsisnotlimitedtothefieldofcomputersciencealone: scipost.organd
f1000research.com outside computer science also release the peer reviews publicly. Subsequently, a number
of studies have released datasets that annotate the reviews (and discussion posts), including annotations
based on the arguments [Hua+19; Fro+20; Ken+21; Kuz+22; Gho+22] or sentiments [CGM20; Bul+20].
3 Can LLMs detect errors in computer science papers?
In this section, we discuss our experiment to investigate whether LLMs can identify flaws in short computer
science papers. Based on our pilot (Appendix A.1) in which GPT-4 is the only model that detected the
error, we use GPT-4 as the LLM for these experiments.
3.1 Methods
We constructed 13 short papers (detailed in Section 3.3). In each of these papers, we deliberately inserted
an error, encompassing mathematical mistakes to conceptual fallacies. We then asked the LLM to identify
if there were any errors in the short paper. We used the following three prompts:
‚Ä¢ Prompt-Direct: You are an expert reviewer for a scientific conference. You will be provided with a
short version of a paper that contains the setting of the paper and the main claims. Please check for
the validity and correctness of these claims, and in particular, report if you can figure out if any of
these claims is false based on the information provided in this short paper. Think step by step when
checking each claim. Here is the short paper: ‚Äú...‚Äù
‚Ä¢ Prompt-OneShot: You are an expert reviewer for a scientific conference. You will be provided with a
short version of a paper that contains the setting of the paper and the main claims. Please check for
the validity and correctness of these claims, and in particular, report if you can figure out if any of
these claims is false based on the information provided in this short paper. Think step by step when
checking each claim.
You will first be provided with an example. Consider this example short paper:
We consider the problem of searching in a list of numbers, where the list is already sorted in
a non-decreasing order. If the number exists in the list, then the search should return the
position of that number in the list (if there are multiple copies of the number in the list, then
it can return any one position). If the number does not exist in the list, then it should output
‚Äòdoes not exist‚Äô. The number to be searched is specified by the user. For this problem, we
develop a new artificial intelligence based algorithm and also prove theoretical guarantees for
it. Specifically, we show that our algorithm requires only sqrt(m) comparisons between pairs
of numbers, where m is the size of the entire list. We also provide an associated information-
theoretic lower bound showing that our bound is tight, i.e., any algorithm will need at least
these many comparisons in the worst case. We conduct a large number of simulations that
confirm this theoretical result. Our simulations vary the size of the list, the values in the list
(drawing them from various distributions), as well as the value to be searched. Our result is
especially important as it is rare for such practical artificial intelligence algorithms to also have
such strong theoretical guarantees. We hope practitioners will take note of this new result.
The review of this short paper should point out the error that searching in a sorted list of numbers
needs only log(m) comparisons (via binary search), and hence the paper‚Äôs claim that they provide a
sqrt(m) worst case lower bound is false.
4
Now, please check for the validity and correctness of the claims in the following short paper, and in
particular, reportif youcanfigureoutif anyoftheseclaims isfalsebasedon theinformationprovided
in this short paper. Think step by step when checking each claim. Here is the short paper: ‚Äú...‚Äù
‚Ä¢ Prompt-Parts: You are an expert reviewer for a scientific conference. You will be provided with a
short version of a paper that contains the setting of the paper and the main claims. Please check for
the validity and correctness of these claims, and in particular, report if you can figure out if any of
these claims is false based on the information provided in this short paper. You will be provided the
paper one set of sentences at a time.
Here is the first set of sentences of the paper: ‚Äú...‚Äù Does this contain any incorrect claim? Think step
by step to reason out your answer.
Here is the next set of sentences of the paper: ‚Äú...‚Äù Based on the context of the previous sentences,
does this contain any incorrect claim or does it invalidate any claim made in the previous sentences of
this paper? Think step by step to reason out your answer.
Note that Prompt-Parts does not provide the entire short paper at once, but instead provides parts of the
paper a few sentences at a time. In particular, for the sentences where the main claims are made, each
providedpartcomprisesasinglesentence. Inthepromptsforboththisandthenextexperiment, thephrase
‚Äúthink step by step‚Äù is inspired by [Koj+23]. For this section‚Äôs experiments, we access the GPT-4 model
through ChatGPT (May 3 and May 12 builds) [Ope23a].
3.2 Summary of results
In Table 1, we provide a summary of our results. We queried the LLM for three responses per prompt and
we provide an evaluation of each response as well as an overall quantification of the performance. In the
‚Äòoverall‚Äô row and column in the table, we consider it as a ‚úì if any of the responses to any of the prompts
was a ‚úì. This is because in practice, one can obtain multiple responses to multiple prompts and flag the
paper if any of them detect an error.
We find that the LLM (GPT-4) can detect the error in 7 of the 13 short papers. Furthermore, in each of
the 6 papers where it failed to detect the error, the paper did not contain the complete proof for the claim,
thereby disallowing the LLM to detect a deductive error, and requiring it to figure out the flaw based on
extraneousknowledge. WealsofindthattheLLMoccasionallyoutputsfalsepositives, thatis, falselyclaims
a correct part of the paper to be incorrect.
Inpassing,itisworthnotingthatwesubsequentlyalsotriedothermodels‚ÄîBard,Vicuna,Koala,Alpaca,
LLaMa, Dolly, OpenAssistant, StableLM. For prompting, we used Prompt-Direct. None of these models
wereabletoidentifytheerrorinanyofthepapers. Additionally,someoftheseothermodelsraisedconcerns
reminiscentoftheproverbial‚ÄúReviewer#2‚Äù,makingunwarrantedcritiquessuchas‚Äútheflawinthispaperis
thattheydonotconductexperimentsinadditiontotheoreticalresults‚Äùorevenmorenonsensicalcomments.
3.3 Paper and response details
In this section, we describe the 13 papers we constructed. Along with each paper, we provide one exam-
ple response from the LLM. Additional responses from the LLM are available at https://github.com/
niharshah/ReviewerGPT2023
3.3.1 Bias/fairness
The following constructed paper rebuts another fictitious paper (Reference 1) that analyzed data involving
Simpson‚Äôs paradox. The paper has a logical fallacy, and we investigate whether the LLM can identify it.
5

================================================================================
PROCESSING: 2408.06292_AI_Scientist.pdf
================================================================================

2024-9-4
The AI Scientist: Towards Fully Automated
Open-Ended Scientific Discovery
1,2,* 3,4,* 1,* 2,(cid:134) 3,4,5,(cid:134) 1,(cid:134)
ChrisLu ,CongLu ,RobertTjarkoLange ,JakobFoerster ,JeffClune andDavidHa
*EqualContribution,1SakanaAI,2FLAIR,UniversityofOxford,3UniversityofBritishColumbia,4VectorInstitute,5CanadaCIFAR
AIChair,(cid:134)EqualAdvising
Oneofthegrandchallengesofartificialgeneralintelligenceisdevelopingagentscapableofconducting
scientific research and discovering new knowledge. While frontier models have already been used
as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they
still conduct only a small part of the scientific process. This paper presents the first comprehensive
frameworkforfullyautomaticscientificdiscovery,enablingfrontierlargelanguagemodels(LLMs)to
performresearchindependentlyandcommunicatetheirfindings. Weintroduce The AI Scientist,
whichgeneratesnovelresearchideas,writescode,executesexperiments,visualizesresults,describes
itsfindingsbywritingafullscientificpaper,andthenrunsasimulatedreviewprocessforevaluation. In
principle,thisprocesscanberepeatedtoiterativelydevelopideasinanopen-endedfashionandadd
themtoagrowingarchiveofknowledge,actinglikethehumanscientificcommunity. Wedemonstrate
theversatilityofthisapproachbyapplyingittothreedistinctsubfieldsofmachinelearning: diffusion
modeling,transformer-basedlanguagemodeling,andlearningdynamics. Eachideaisimplemented
anddevelopedintoafullpaperatameagercostoflessthan$15perpaper,illustratingthepotentialfor
ourframeworktodemocratizeresearchandsignificantlyacceleratescientificprogress. Toevaluatethe
generatedpapers,wedesignandvalidateanautomatedreviewer,whichweshowachievesnear-human
performance in evaluating paper scores. The AI Scientist can produce papers that exceed the
acceptancethresholdatatopmachinelearningconferenceasjudgedbyourautomatedreviewer. This
approach signifies the beginning of a new era in scientific discovery in machine learning: bringing
thetransformativebenefitsofAIagentstotheentireresearchprocessofAIitself,andtakinguscloser
to a world where endless affordable creativity and innovation can be unleashed on the world‚Äôs most
challengingproblems. Ourcodeisopen-sourcedathttps://github.com/SakanaAI/AI-Scientist.
1. Introduction
The modern scientific method (Chalmers, 2013; Dewey, 1910; Jevons, 1877) is arguably one of the
greatest achievements of the Enlightenment. Traditionally, a human researcher collects background
knowledge, drafts a set of plausible hypotheses to test, constructs an evaluation procedure, collects
evidenceforthedifferenthypotheses,andfinallyassessesandcommunicatestheirfindings. Afterward,
the resulting manuscript undergoes peer review and subsequent iterations of refinement. This
procedure has led to countless breakthroughs in science and technology, improving human quality of
life. However,thisiterativeprocessisinherentlylimitedbyhumanresearchers‚Äôingenuity,background
knowledge,andfinitetime. Attemptingtoautomategeneralscientificdiscovery(Langley,1987,2024;
WaltzandBuchanan,2009)hasbeenalongambitionofthecommunitysinceatleasttheearly70s,with
computer-assisted works like the Automated Mathematician (Lenat, 1977; Lenat and Brown, 1984)
andDENDRAL(BuchananandFeigenbaum,1981). InthefieldofAI,researchershaveenvisionedthe
possibilityofautomatingAIresearchusingAIitself(Ghahramani,2015;Schmidhuber,1991,2010a,b,
2012), leading to ‚ÄúAI-generating algorithms‚Äù (Clune, 2019). More recently, foundation models
have seen tremendous advances in their general capabilities (Anthropic, 2024; Google DeepMind
GeminiTeam,2023;LlamaTeam,2024;OpenAI,2023),buttheyhaveonlybeenshowntoaccelerate
individualpartsoftheresearchpipeline,e.g. thewritingofscientificmanuscripts(Altm√§eetal.,2023;
Correspondingauthor(s):ChrisLu(chrislu@sakana.ai),CongLu(conglu@cs.ubc.ca),andRobertTjarkoLange(robert@sakana.ai)
4202
peS
1
]IA.sc[
3v29260.8042:viXra
TheAIScientist:TowardsFullyAutomatedOpen-EndedScientificDiscovery
Dinu et al., 2024; Ifargan et al., 2024; Majumder et al., 2024), as a muse to brainstorm ideas (Baek
et al., 2024; Girotra et al., 2023; Wang et al., 2024b), or aides to coding (Gauthier, 2024). To date,
the community has yet to show the possibility of executing entire research endeavors without human
involvement.
Traditional approaches to automating research projects have so far relied on carefully constrain-
ing the search space of potential discoveries, which severely limits the scope of exploration and
requires substantial human expertise and design. For example, significant advancements in materials
discovery (Merchant et al., 2023; Pyzer-Knapp et al., 2022; Szymanski et al., 2023) and synthetic
biology (Hayes et al., 2024; Jumper et al., 2021) have been achieved by restricting exploration to
well-characterized domains with predefined parameters, which allows for targeted progress but
limits broader, open-ended discovery and addressing only a subset of the scientific process, without
encompassing tasks such as manuscript preparation. Within the field of machine learning itself,
researchautomationhas largelybeenrestrictedtohyperparameter andarchitecturesearch(Heetal.,
2021;Hutteretal.,2019;Luetal.,2022b;Wanetal.,2021,2022)oralgorithmdiscovery(Aletetal.,
2020;Chenetal.,2024b;Kirschetal.,2019;Langeetal.,2023a,b;Luetal.,2022a;Metzetal.,2022)
within a hand-crafted search space. Recent advances in LLMs have shown the potential to extend the
search space to more generalized, code-level solutions (Faldor et al., 2024; Lehman et al., 2022; Lu
et al., 2024a; Ma et al., 2023). However, these approaches remain constrained by rigorously-defined
search spaces and objectives, which limit the breadth and depth of possible discoveries.
In this paper, we introduce The AI Scientist, the first fully automated and scalable pipeline
for end-to-end paper generation, enabled by recent advances in foundation models. Given a broad
researchdirectionandasimpleinitialcodebase, The AI Scientist seamlesslyperformsideation,a
literaturesearch,experimentplanning,experimentiterations,manuscriptwriting,andpeerreviewing
to produce insightful papers. Furthermore, in principle The AI Scientist can run in an open-
ended loop, building on its previous scientific discoveries to improve the next generation of ideas.
This allows us to speed up the slow nature of scientific iteration at a surprisingly low financial cost
(‚àº$15/paper) and represents a step towards turning the world‚Äôs ever-increasing computing resources
into the scientific breakthroughs needed to tackle the core challenges of the 21st century. Here, we
focus on Machine Learning (ML) applications, but this approach can more generally be applied to
almostanyotherdiscipline,e.g. biologyorphysics,givenanadequatewayofautomaticallyexecuting
experiments (Arnold, 2022; Kehoe et al., 2015; Zucchelli et al., 2021).
ByleveragingmodernLLMframeworkslikechain-of-thought(Weietal.,2022)andself-reflection(Shinn
et al., 2024) to improve decision-making, The AI Scientist is able to generate its own scientific
ideasandhypotheses,aswellasaplanfortestingthemwithexperiments. Next, The AI Scientist
implements plan-directed code-level changes to the experiment ‚Äútemplate‚Äù using the state-of-the-art
coding assistant Aider (Gauthier, 2024), and executes experiments to collect a set of computational
results, which are in turn used to draft a scientific paper. The AI Scientist then performs an
automated paper-reviewing process using guidelines from a standard machine learning conference.
Finally, The AI Scientist adds the completed ideas and reviewer feedback to its archive of
scientific findings, and the process repeats. Crucially, the generated paper and experimental artifacts
The AI Scientist produces allow us to easily interpret and judge its findings post-hoc, allowing
human scientists to also benefit from what is learned.
Our contributions are summarized as follows:
1. Weintroducethefirstend-to-endframeworkforfullyautomatedscientificdiscoveryinMachine
Learning research, enabled by frontier LLMs (Section 3). This fully automated process includes
ideageneration,experimentdesign,execution,andvisualizingandwritinguptheresultsintoa
full manuscript.
2
TheAIScientist:TowardsFullyAutomatedOpen-EndedScientificDiscovery
Figure 1 | Conceptual illustration of The AI Scientist, an end-to-end LLM-driven scientific discovery
process. The AI Scientist firstinventsandassessesthenoveltyofasetofideas. Itthendetermineshowto
testthehypotheses,includingwritingthenecessarycodebyeditingacodebasepoweredbyrecentadvancesin
automatedcodegeneration. Afterward,theexperimentsareautomaticallyexecutedtocollectasetofresults
consisting of both numerical scores and visual summaries (e.g. plots or tables). The results are motivated,
explained,andsummarizedinaLaTeXreport. Finally, The AI Scientist generatesanautomatedreview,
according to current practice at standard machine learning conferences. The review can be used to either
improvetheprojectorasfeedbacktofuturegenerationsforopen-endedscientificdiscovery.
2. Toassessthequalityofthegeneratedpapers,weintroduceafoundationmodel-basedreviewing
processinSection4. Thisprocessachievesnear-human-levelperformanceacrossmultipleevalu-
ationmetrics(e.g.65%vs.66%balancedaccuracy)whenevaluatedonICLR2022OpenReview
data. The reviews further enable The AI Scientist to select the best ideas for ‚Äúpublication‚Äù
to an ever-growing archive of scientific discoveries, and the process can be repeated to build on
these discoveries, just as in the human scientific community.
3. The AI Scientist can generate hundreds of interesting, medium-quality papers over the
courseofaweek. Inthisreport,wefocusonasubsetofthesepapers,highlightingnovelinsights
in diffusion modeling, language modeling, and grokking. We perform an in-depth case study
into one selected paper in Section 5, and present aggregate results in Section 6.
4. We conclude the paper with an extensive discussion on the limitations, ethical considerations,
and future outlook of our approach in Sections 8 and 9.
2. Background
Large Language Models. In this paper, we build our automated scientist from autoregressive large
language models (LLMs, Anthropic (2023); Google DeepMind Gemini Team (2023); Llama Team
(2024);OpenAI(2023);Zhuetal.(2024))whichlearntogeneratetextcompletionsbymodelingthe
conditional probability of a new token (similar to a word) given the preceding tokens, ùëù(ùë• |ùë• ;ùúÉ),
ùë° <ùë°
and sampling at test-time. Together with vast data and model scaling, this enables LLMs to not
only generate coherent text, but crucially also exhibit human-like abilities, including commonsense
knowledge (Talmor et al., 2019), reasoning (Wei et al., 2022), and the ability to write code (Chen
et al., 2021; Xu et al., 2022).
LLM Agent Frameworks. Typical applications of LLMs often involve embedding the model into
an ‚Äúagent‚Äù (Wang et al., 2024a) framework, including the following possibilities: the structuring of
3
TheAIScientist:TowardsFullyAutomatedOpen-EndedScientificDiscovery
language queries (e.g. few-shot prompting (Brown et al., 2020)), encouraging reasoning traces (e.g.
chain-of-thought (Wei et al., 2022)), or asking the model to iteratively refine its outputs (e.g., self-
reflection(Shinnetal.,2024)). Theseleveragethelanguagemodel‚Äôsabilitytolearnin-context(Olsson
etal.,2022)andcangreatlyimproveitsperformance,robustnessandreliabilityonmanytasks.
Aider: An LLM-Based Coding Assistant. Our automated scientist directly implements ideas in code
and uses a state-of-the-art open-source coding assistant, Aider (Gauthier, 2024). Aider is an agent
framework that is designed to implement requested features, fix bugs, or refactor code in existing
codebases. While Aider can in principle use any underlying LLM, with frontier models it achieves a
remarkable success rate of 18.9% on the SWE Bench (Jimenez et al., 2024) benchmark, a collection
of real-world GitHub issues. In conjunction with new innovations added in this work, this level of
reliability enables us, for the first time, to fully automate the ML research process.
3. The AI Scientist
Overview. The AI Scientist has three main phases (Figure 1): (1) Idea Generation, (2) Ex-
perimental Iteration, and (3) Paper Write-up. After the write-up, we introduce and validate an
LLM-generated review to assess the quality of the generated paper (Section 4). We provide The AI
Scientist with a starting code template that reproduces a lightweight baseline training run from a
popular model or benchmark. For example, this could be code that trains a small transformer on the
worksofShakespeare(Karpathy,2022),aclassicproof-of-concepttrainingrunfromnaturallanguage
processing that completes within a few minutes. The AI Scientist is then free to explore any
possible research direction. The template also includes a LaTeX folder that contains style files and
section headers, along with simple plotting code. We provide further details on the templates in
Section 6, but in general, each run starts with a representative small-scale experiment relevant to
the topic area. The focus on small-scale experiments is not a fundamental limitation of our method,
but simply for computational efficiency reasons and compute constraints on our end. We provide the
prompts for all stages in Appendix A.
1. Idea Generation. Givenastartingtemplate, The AI Scientist first‚Äúbrainstorms‚Äùadiverseset
of novel research directions. We take inspiration from evolutionary computation and open-endedness
research (Brant and Stanley, 2017; Lehman et al., 2008; Stanley, 2019; Stanley et al., 2017) and
iterativelygrowanarchiveofideasusingLLMsasthemutationoperator(Faldoretal.,2024;Lehman
et al., 2022; Lu et al., 2024b; Zhang et al., 2024). Each idea comprises a description, experiment
executionplan,and(self-assessed)numericalscoresofinterestingness,novelty,andfeasibility. Ateach
iteration,wepromptthelanguagemodeltogenerateaninterestingnewresearchdirectionconditional
ontheexistingarchive,whichcanincludethenumericalreviewscoresfromcompletedpreviousideas.
We use multiple rounds of chain-of-thought (Wei et al., 2022) and self-reflection (Shinn et al., 2024)
to refine and develop each idea. After idea generation, we filter ideas by connecting the language
model with the Semantic Scholar API (Fricke, 2018) and web access as a tool (Schick et al., 2024).
This allows The AI Scientist to discard any idea that is too similar to existing literature.
2. Experiment Iteration. Given an idea and a template, the second phase of The AI Scientist
first executes the proposed experiments and then visualizes its results for the downstream write-up.
The AI Scientist uses Aider to first plan a list of experiments to run and then executes them
in order. We make this process more robust by returning any errors upon a failure or time-out (e.g.
experiments taking too long to run) to Aider to fix the code and re-attempt up to four times.
After the completion of each experiment, Aider is then given the results and told to take notes in
the style of an experimental journal. Currently, it only conditions on text but in future versions, this
could include data visualizations or any modality. Conditional on the results, it then re-plans and
implements the next experiment. This process is repeated up to five times. Upon completion of
4
TheAIScientist:TowardsFullyAutomatedOpen-EndedScientificDiscovery
experiments, Aider is prompted to edit a plotting script to create figures for the paper using Python.
The AI Scientist makes a note describing what each plot contains, enabling the saved figures
and experimental notes to provide all the information required to write up the paper. At all steps,
Aider sees its history of execution.
Note that, in general, the provided initial seed plotting and experiment templates are small, self-
contained files. The AI Scientist frequently implements entirely new plots and collects new
metrics that are not in the seed templates. This ability to arbitrarily edit the code occasionally leads
to unexpected outcomes (Section 8).
3. Paper Write-up. The third phase of The AI Scientist produces a concise and informative
write-up of its progress in the style of a standard machine learning conference proceeding in LaTeX.
We note that writing good LaTeX can even take competent human researchers some time, so we take
several steps to robustify the process. This consists of the following:
(a) Per-Section Text Generation: The recorded notes and plots are passed to Aider, which is
prompted to fill in a blank conference template section by section. This goes in order of
introduction, background, methods, experimental setup, results, and then the conclusion (all
sections apart from the related work). All previous sections of the paper it has already written
are in the context of the language model. We include brief tips and guidelines on what each
section should include, based on the popular ‚ÄúHow to ML Paper‚Äù guide, and include details in
Appendix A.3. At each step of writing, Aider is prompted to only use real experimental results in
theformofnotesandfiguresgeneratedfromcode,andrealcitationstoreducehallucination. Each
section is initially refined with one round of self-reflection (Shinn et al., 2024) as it is being
written. Aiderispromptedtonotincludeanycitationsinthetextatthisstage,andfillinonlya
skeleton for the related work, which will be completed in the next stage.
(b) Web Search for References: In a similar vein to idea generation, The AI Scientist is
allowed 20 rounds to poll the Semantic Scholar API looking for the most relevant sources to
compare and contrast the near-completed paper against for the related work section. This
process also allows The AI Scientist to select any papers it would like to discuss and
additionally fill in any citations that are missing from other sections of the paper. Alongside
each selected paper, a short description is produced of where and how to include the citation,
which is then passed to Aider. The paper‚Äôs bibtex is automatically appended to the LaTeX file to
guarantee correctness.
(c) Refinement: After the previous two stages, The AI Scientist has a completed first draft,
but can often be overly verbose and repetitive. To resolve this, we perform one final round of
self-reflection section-by-section, aiming to remove any duplicated information and streamline
the arguments of the paper.
(d) Compilation: Once the LaTeX template has been filled in with all the appropriate results, this
is fed into a LaTeX compiler. We use a LaTeX linter and pipe compilation errors back into Aider
so that it can automatically correct any issues.
4. Automated Paper Reviewing
An LLM Reviewer Agent. A key component of an effective scientific community is its reviewing
system, which evaluates and improves the quality of scientific papers. To mimic such a process using
large language models, we design a GPT-4o-based agent (OpenAI, 2023) to conduct paper reviews
based on the Neural Information Processing Systems (NeurIPS) conference review guidelines. The
review agent processes the raw text of the PDF manuscript using the PyMuPDF parsing library. The
output contains numerical scores (soundness, presentation, contribution, overall, confidence), lists of
weaknesses and strengths as well as a preliminary binary decision (accept or reject). These decisions
5

================================================================================
PROCESSING: 2504.08066_AI_Scientist_v2.pdf
================================================================================

2025-4-14
The AI Scientist-v2: Workshop-Level
Automated Scientific Discovery via Agentic
Tree Search
YutaroYamada1,*,RobertTjarkoLange1,*,CongLu1,2,3,*,ShengranHu1,2,3,ChrisLu4,JakobFoerster4,Jeff
Clune2,3,5,(cid:134) andDavidHa1,(cid:134)
*EqualContribution,1SakanaAI,2UniversityofBritishColumbia,3VectorInstitute,4FLAIR,UniversityofOxford,5CanadaCIFAR
AIChair,(cid:134)EqualAdvising
AIisincreasinglyplayingapivotalroleintransforminghowscientificdiscoveriesaremade. Weintroduce
The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI-
generatedpeer-review-acceptedworkshoppaper. Thissystemiterativelyformulatesscientifichypotheses,
designsandexecutesexperiments,analyzesandvisualizesdata,andautonomouslyauthorsscientific
manuscripts. Comparedtoitspredecessor(v1,Luetal.,2024), The AI Scientist-v2 eliminatesthe
relianceonhuman-authoredcodetemplates,generalizeseffectivelyacrossdiversemachinelearning
domains,andleveragesanovelprogressiveagentictree-searchmethodologymanagedbyadedicated
experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a
Vision-LanguageModel(VLM)feedbackloopforiterativerefinementofcontentandaestheticsofthe
figures. Weevaluated The AI Scientist-v2 bysubmittingthreefullyautonomousmanuscriptsto
apeer-reviewedICLRworkshop. Notably,onemanuscriptachievedhighenoughscorestoexceedthe
averagehumanacceptancethreshold,markingthefirstinstanceofafullyAI-generatedpapersuccessfully
navigatingapeerreview. ThisaccomplishmenthighlightsthegrowingcapabilityofAIinconductingall
aspectsofscientificresearch. Weanticipatethatfurtheradvancementsinautonomousscientificdiscovery
technologieswillprofoundlyimpacthumanknowledgegeneration,enablingunprecedentedscalability
inresearchproductivityandsignificantlyacceleratingscientificbreakthroughs,greatlybenefitingsociety
at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to
fosterthefuturedevelopmentofthistransformativetechnology. WealsodiscusstheroleofAIinscience,
includingAIsafety.
1. Introduction
Automated scientific discovery empowered by artificial intelligence (AI) has garnered considerable
attentioninrecentyears(Cornelioetal.,2023;Giletal.,2014;Kingetal.,2009;Kitano,2021;Wang
et al., 2023; Xu et al., 2021). The development of end-to-end frameworks capable of autonomously
formulatinghypotheses,performingexperiments,analyzingresults,andauthoringmanuscriptscould
fundamentally transform the scientific process. A notable recent advance in this direction is The AI
Scientist-v1 (Lu et al., 2024), which demonstrated the feasibility of a fully automated scientific
workflow and downstream manuscript production. However, significant limitations constrained its
broad applicability and autonomy. Specifically, it relied heavily on human-authored code templates
requiringmanualefforttocreateanewtemplateforeachnewtopicarea. Furthermore,itslinearand
shallow experimentation approach prevented deeper exploration of scientific hypotheses.
Inthispaper, weintroduce The AI Scientist-v2,asubstantiallyimprovedsuccessorthatdirectly
addresses these limitations. Our contributions are threefold. First, we eliminate the dependency
on human-provided code templates, significantly increasing the system‚Äôs autonomy and ability to
be deployed out of the box across multiple machine learning domains. Second, we introduce an
experiment manager agent coupled with a novel agentic tree-search algorithm, enabling deeper and
Correspondingauthor(s):YutaroYamada(yutaroyamada@sakana.ai),RobertTjarkoLange(robert@sakana.ai),andCongLu
(cong@sakana.ai).
5202
rpA
01
]IA.sc[
1v66080.4052:viXra
The AI Scientist-v2:Workshop-LevelAutomatedScientificDiscoveryviaAgenticTreeSearch
Table1 | Comparison of AI Scientist Versions. Comparisonhighlightskeyadvancementsin The AI
Scientist-v2, including autonomous code generation via tree search, enhanced VLM integration
for feedback during experiments and manuscript review, and evaluation through formal peer review.
Feature Codebase Execution Parallel VLM HumanResult
Drafting Planning Experiments Reviewer Evaluation
The AI Scientist-v1 Topic-Specific Linear ‚úó ‚úó NotSubmitted
The AI Scientist-v2 Domain-General Tree-Based ‚úì ‚úì WorkshopAcceptance-Worthy
moresystematicexplorationofcomplexhypotheses. Third,weenhancethereviewingandrefinement
stages by integrating a Vision-Language Model (VLM)-based feedback mechanism, improving the
quality, clarity, and alignment of generated figures, captions, and text interpretation. To rigorously
evaluate the capabilities and limitations of fully autonomous manuscript generation, we conducted
a controlled experiment: three manuscripts entirely generated by The AI Scientist-v2 were
submitted to a peer-reviewed workshop at ICLR. Remarkably, one manuscript achieved an average
reviewer score of 6.33 (placing it roughly in the top 45% of submissions) and would have been
accepted after meta-review were it human-generated, thus becoming the first fully AI-generated
manuscript to successfully pass a peer-review process.
The accepted paper investigates whether incorporating an explicit compositional regularization term
intoneuralnetworktrainingcanimprovecompositionalgeneralization. Specifically,itpenalizeslarge
deviations between embeddings of successive time steps in sequence models, hypothesizing that
this encourages compositionality. The approach is evaluated using synthetic arithmetic expression
datasets, but it is found that compositional regularization does not yield significant improvements
and occasionally harms performance. The workshop reviewers appreciated the paper for clearly
identifying the challenges of effective compositional regularization and reporting on negative results.
However, they collectively highlighted shortcomings, including insufficient justification and intuitive
explanationsforwhythechosenregularizationmethodwouldenhancecompositionality. Ourpersonal
assessment (detailed further in ¬ß4) highlights several additional potential improvements in method
description(e.g.,makingclearexactlywhichcomponentofthenetworkisbeingregularized),potential
dataset overlap issues, and inaccuracies in figure captions. Overall, reviewers viewed the paper as an
interestingandtechnicallysoundworkshopcontributionthatneedsfurtherdevelopmentandbroader
experimentation to reach conference-level rigor.
This report provides an in-depth outline of the developed methodological advances, analysis of the
workshop-submitted papers, and a discussion on the ethical and safety considerations of systems like
The AI Scientist-v2. Our overall contributions are as follows:
1. Weintroduce The AI Scientist-v2,anautomatedscientificdiscoveryframeworkenhanced
byagentictreesearch,VLMfeedback,andparallelexperimentexecution. Ittherebysignificantly
improves the autonomy, flexibility, and scientific exploration depth of previous systems.
2. We demonstrate, for the first time, that an AI-generated manuscript can successfully pass peer
review at a recognized machine learning workshop, marking a critical milestone for AI science.
3. We conduct comprehensive internal evaluations and analyses of both peer-review feedback and
our system‚Äôs outputs, providing insights into the strengths, weaknesses, and current status of
AI-generated manuscripts relative to traditional human-authored scientific publications.
4. We open-source the full codebase for The AI Scientist-v2 and the ICLR 2025 workshop
experiment data, encouraging further exploration by the research community and advancing a
discussion regarding AI‚Äôs evolving role in science‚Äìin the open.
2
The AI Scientist-v2:Workshop-LevelAutomatedScientificDiscoveryviaAgenticTreeSearch
Figure 1 | The AI Scientist-v2 Workflow. The workflow consists of several phases covering
automated idea generation, experiment execution, figure visualization, manuscript writing, and
reviewing. Unlike the initial version, The AI Scientist-v2 removes the dependency on human-
coded templates. Instead, it employs agentic tree search (managed by an Experiment Progress
Manager across several stages, orange) to generate and refine code implementations. Subsequent
experimentation leverages the best-performing code checkpoints (nodes) from the tree search to
iteratively test various research hypotheses.
2. Background
The AI Scientist-v1 (Luetal.,2024)introducedthefirstAIsystemthatentirelyautomatesscien-
tificdiscoveryandthepresentationofitsresults. Givenabaselinecodetemplate,itautonomouslywrote
code, executed experiments, visualized outcomes, and produced a complete scientific manuscript.
However, despite representing a significant step forward, The AI Scientist-v1 was subject to
limitations. Foremost among these was its reliance on human-crafted baseline code templates, signifi-
cantly constraining its autonomy and hindering unconstrained out-of-the-box deployability. Instead,
human effort was still required to draft an initial base experiment outline in code. Additionally, the
experimentation process followed a strictly linear hypothesis-testing routine, limiting depth and
exploration flexibility, especially when addressing complex research questions.
Language Model Agent Scaffolding. To further enhance LLM performance on complex reasoning
tasks, researchers have developed agentic scaffolding frameworks, each with distinct advantages
and limitations. For example, Reflexion (Shinn et al., 2024) enables models to iteratively reflect
on previous responses, encouraging self-improvement through critical evaluation of past outputs;
it improves robustness, but can introduce computational overhead and slower inference. Another
promising direction is the integration of tree-search strategies with LLMs (Jiang et al., 2025), allow-
ing structured exploration of reasoning paths. This approach enhances systematic reasoning and
comprehensiveness, though at the cost of increased complexity, higher computational demands, and
challenges in scalability.
Tree Search with Large Language Models. We empirically observed that automated research
conducted by The AI Scientist-v1 often resulted in short-sighted experimentation. The human-
driven scientific process, on the other hand, relies on open-ended hypothesis generation, stepping-
stone collection, and iterative hypothesis refinement. Recent advances using code generation as
an action space have opened new opportunities for LLM-driven automated workflows (Wang et al.,
2024). AIDE (Jiang et al., 2025) combines LLM-based code generation with tree search, demon-
3
The AI Scientist-v2:Workshop-LevelAutomatedScientificDiscoveryviaAgenticTreeSearch
strating state-of-the-art performance on the MLEBench benchmark (Chan et al., 2025), designed for
machine learning engineering tasks. In AIDE, each node represents a potential solution state with a
corresponding scalar evaluation score (e.g., validation accuracy). Nodes are iteratively selected for
further debugging or refinement based on these scores. Inspired by this approach, we integrate a
similar tree search-based exploration strategy within our automated scientific discovery framework,
adaptingitspecificallytothemulti-stagenatureofscientificexperimentation,asdetailedin¬ß3.
3. The AI Scientist-v2
We now describe the major innovations introduced in The AI Scientist-v2 relative to The AI
Scientist-v1 (Lu et al., 2024). The most significant improvement is the move towards greater
autonomy and generalization, starting a more general idea generation phase (¬ß3.1) and eliminating
the reliance on fixed, human-authored template code for experimentation. This process begins with
generalized idea generation, producing an initial concept, which then feeds into the experimentation
phase (¬ß3.2). To manage this, we introduce two critical features in the experimentation phase:
coarse-grained experiment management and agentic tree search-based exploration. Additionally, we
integrate Vision Language Models (VLMs) into the experimental and review phases (¬ß3.4). Finally,
we streamline the manuscript writing phase by replacing the incremental, Aider-based (Gauthier,
2024) iterative writing approach of The AI Scientist-v1 with a simpler, single-pass generation
followed by a separate reflection stage powered by reasoning models such as o1 (OpenAI, 2024). We
includeafulllistofsamplinghyperparametersandmodelsusedinAppendixAandthepromptsused
for The AI Scientist-v2 in Appendix B.
3.1. More General Idea Generation
A key conceptual shift in The AI Scientist-v2 is the approach to research idea generation.
Unlike the predecessor system, which primarily focused on proposing incremental modifications or
extensionsbasedonanexistingcodebase, The AI Scientist-v2 adoptsaprocessthatbeginsata
higher level of abstraction. The system is prompted to engage in more open-ended thinking about
potential research directions, hypotheses, and experimental designs, akin to formulating a research
abstract or grant proposal before committing to a specific implementation.
This approach encourages the exploration of potentially more novel or foundational ideas, rather
than being constrained by the structure and topics of pre-existing code. It aligns more closely with
howresearchersoftendevelopbroaderresearchvisions,startingwithabstractconceptsandassessing
novelty and feasibility before diving into specific implementations. Crucially, this generalized idea
generationphaseintegratesliteraturereviewtools,suchasSemanticScholar,intheloop. Thesystem
can query the literature database during the idea formulation process to assess the novelty of a
proposed concept and identify relevant prior work. This allows for more informed decisions about
pursuingaparticularresearchavenue,ensuringideasaregroundedintheexistingscientificlandscape
from the outset, rather than relying solely on post-hoc checks.
3.2. Removing Template Dependency
Following the improved idea generation phase, The AI Scientist-v2 proceeds with experimen-
tation. Beyond the code-conditioned idea generation, The AI Scientist-v1 also depended on
the predefined template code as a starting baseline implementation. The LLM-driven code changes
were then limited to sequential code adaptations. We now outline our strategy for eliminating this
limitation, thus improving the system‚Äôs flexibility and autonomy.
3.2.1. Experiment Progress Manager
Real-worldscientificexperimentationtypicallyproceedsthroughdistinctstages,frominitialfeasibility
assessments to detailed ablation analyses. To emulate this structured approach, we introduce an
4
The AI Scientist-v2:Workshop-LevelAutomatedScientificDiscoveryviaAgenticTreeSearch
experiment progress manager agent that coordinates four clearly defined stages of scientific
experimentation:
Stage 1 PreliminaryInvestigation: Establishinginitialfeasibilityandcorrectnessthroughaminimal
working prototype based on the generated research idea.
Stage 2 Hyperparameter Tuning: Refining the initial implementation by optimizing critical hyper-
parameters (e.g., learning rate, epochs) to create a robust experimental baseline.
Stage 3 Research Agenda Execution: Systematically implementing the core research agenda based
on the tuned baseline.
Stage 4 Ablation Studies: Systematically assessing the importance of various research components,
providing rigorous support for the main experimental findings.
Each stage has explicit stopping criteria. Stage 1 concludes when a basic working prototype is
successfully executed. Stage 2 ends when experiments stabilize, as indicated by convergence in
training curves and successful execution across at least two datasets. Stages 3 and 4 conclude when
the allocated computational budget is exhausted. Stage 3 also includes a check for experiment
duration‚Äìifrunsfinishmuchfasterthanthepre-allocatedruntime,thesystemsuggestsincreasingthe
complexity of experiments.
After each stage, the experiment manager selects the best-performing node using a dedicated LLM
evaluator (see next section) based on clearly articulated criteria. This selected node is then carried
forwardtoseedthesubsequentexperimentationstage. Themanageralsorecordscheckpointsateach
stage‚Äôs completion. To ensure scientific rigor and reproducibility, the experiment manager launches
multiple replications of the selected best experiments at the conclusion of each stage. These repeated
runs provide statistics (mean and standard deviation) for figures and reported results.
3.2.2. Parallelized Agentic Tree Search
The AI Scientist-v1 operated strictly linearly, where each code refinement directly built on
the immediately preceding experiment. In contrast, The AI Scientist-v2 adopts a significantly
more flexible and exploratory approach inspired by recent successes in integrating tree search with
LLM-drivenworkflows(Chanetal.,2025;Jiangetal.,2025;Wijketal.,2024)andresearchonopen-
endedness(Clune,2019;MouretandClune,2015). Weincorporatethisagentictreesearchapproach
across all four experimentation stages outlined in ¬ß3.2.1, enabling deeper and more systematic
exploration of scientific hypotheses.
Each experimental node within our tree-based framework undergoes the following execution cycle:
An LLM first generates both a concrete experimentation plan and the associated Python code to
implement the experiment. The generated code is immediately executed in a Python interpreter.
If execution encounters an error, the error message is recorded, and the node is marked as buggy,
ending the current execution cycle for that node. If execution succeeds, the experiment proceeds to
the plotting phase.
During each experiment, the system is instructed to save all relevant experimental outputs (training
and validation metrics, losses, etc.) into structured numpy files. In the plotting phase, The AI
Scientist-v2 reads these stored results and the code, generating visualizations that summarize
and illustrate the findings clearly. These visualizations are subsequently passed to a Vision-Language
Model (VLM) for critique. Any issues flagged by the VLM (such as unclear labels, missing legends, or
misleading visualizations) result in the node being marked as buggy, and this feedback is recorded
for future debugging. Nodes that successfully execute and pass the VLM review without issue are
designated as non-buggy.
We define each node as a collection comprising an experiment script (e.g., a Python file), a textual
5

================================================================================
PROCESSING: 2508.14146_MMReview.pdf
================================================================================

MMReview: A Multidisciplinary and Multimodal Benchmark for
LLM-Based Peer Review Automation
XianGao1,JiachengRuan1,ZongyunZhang1,JingshengGao1,TingLiu1,YuzhuoFu1,
1ShanghaiJiaoTongUniversity,
gaoxian@sjtu.edu.cn
Abstract LLMshavemadeautomatedpeerreviewincreas-
inglyviable,asthesemodelsshowstrongreason-
Withtherapidgrowthofacademicpublications, ingabilitiesandcanofferconstructivefeedbackon
peerreviewhasbecomeanessentialyettime- academicmanuscripts(LiuandShah,2023;Zhao
consuming responsibility within the research
et al., 2024; Zhuang et al., 2025), partially alle-
community. LargeLanguageModels(LLMs)
viating reviewer burden. Yet, current evaluations
haveincreasinglybeenadoptedtoassistinthe
of LLM-generated reviews focus mainly on final
generationofreviewcomments;however,cur-
outputs,lackingin-depthanalysisofthereasoning
rent LLM-based review tasks lack a unified
evaluationbenchmarktorigorouslyassessthe processesbehindmodeljudgments. Additionally,
models‚Äôabilitytoproducecomprehensive,ac- most studies concentrate on AI papers with pub-
curate,andhuman-alignedassessments,partic- liclyavailabletext,overlookingthemultimodalna-
ularlyinscenariosinvolvingmultimodalcon- tureofacademicpapers,suchasfiguresandtables,
tentsuchasfiguresandtables. Toaddressthis
andtheevaluationofLLMsinreviewingresearch
gap, we propose MMReview, a comprehen-
acrossbroaderscientificdomains.
sivebenchmarkthatspansmultipledisciplines
To address the aforementioned challenges, we
and modalities. MMReview includes multi-
proposeMMReview,acomprehensivebenchmark
modalcontentandexpert-writtenreviewcom-
ments for 240 papers across 17 research do- forpeerreviewgenerationthatspansmultipledis-
mainswithinfourmajoracademicdisciplines: ciplinesandmodalities. MMReviewincorporates
Artificial Intelligence, Natural Sciences, En- three distinct types of input modalities: textual
gineeringSciences, andSocialSciences. We
content from manuscripts, figures and tables em-
designatotalof13tasksgroupedintofourcore
beddedwithinthepapers,andrenderedPDFpages
categories,aimedatevaluatingtheperformance
convertedintoimages. Thesedataspan17research
ofLLMsandMultimodalLLMs(MLLMs)in
domains across 4 disciplinary categories. To ob-
step-wisereviewgeneration, outcomeformu-
lation,alignmentwithhumanpreferences,and tain high-quality peer review samples for evalua-
robustness to adversarial input manipulation. tionpurposes,wedevelopedamulti-modelcollab-
Extensiveexperimentsconductedon18open- orative pipeline for data filtering and generation.
sourcemodelsand3advancedclosed-source Specifically, wefirstcuratedatotalof51,881pa-
models demonstrate the thoroughness of the
perswithassociatedreviews;then,wefilteredthe
benchmark. WeenvisionMMReviewasacrit-
collectedseeddatasetDtoobtainhigh-qualitypa-
ical step toward establishing a standardized
pers while maintaining a relatively balanced dis-
foundationforthedevelopmentofautomated
tribution;subsequently,weextractedreferencean-
peerreviewsystems.
swersfromhumanreviews. Finally,weconducted
1 Introduction manualverificationtocorrecterrors,resultingina
curatedsetof240samplesthatserveasthefoun-
Peerreviewisessentialtoscholarlypublishing,en- dationfortaskconstruction. Buildinguponthese
suring research quality and enhancing academic samples, we introduce 4 thematic categories en-
writing. However, the growing volume of sub- compassing13diversetasks,eachdesignedtothor-
missions has strained the traditional review pro- oughlyassessthecapabilitiesofLLMsinstep-wise
cess,leadingtoinefficienciesandlimitedreviewer reviewgeneration,outcomeformulation,alignment
availability(Kimetal.,2025),whichrestrictsfeed- withhumanpreferences,androbustnesstoadver-
back and delays review outcomes. Advances in sarialinputmanipulation. Weconductcomprehen-
1
5202
tcO
8
]LC.sc[
4v64141.8052:viXra
siveexperimentson16open-sourcemodelsaswell prompting(D‚ÄôArcyetal.,2024)togeneratecom-
as5state-of-the-artclosed-sourcemodels,includ- prehensive feedback. Nonetheless, these efforts
ingGPT-4oandClaude-4-Sonnet,across13tasks. focussolelyontextualreviews,neglectingthevi-
Theresultshighlightthecomprehensivenatureof talroleofmultimodalcontent,suchasfiguresand
the MMReview benchmark and uncover several tables,andlackrigorousanalysisofthereasoning
keyfindings,offeringinsightsforfutureresearch processesbehindLLM-generatedcritiques.
onLLM-basedautomatedacademicpeerreview.
2.2 EvaluationforLLM-basedPeerReview
Theprimarycontributionsofthispapercanbe
summarizedasfollows: Prior studies (Shen et al., 2022; Yu et al., 2024;
Gao et al., 2024; Tan et al., 2024; Gao et al.,
‚Ä¢ We introduce MMReview, the first compre- 2025) have predominantly evaluated the quality
hensiveevaluationbenchmarkforautomated ofLLM-generatedpeerreviewcommentsbymea-
academicpeerreviewusingLLMs,spanning suringtheircorrelationorsimilaritywithhuman-
multiple disciplines and modalities. Built written reviews using automated metrics such
uponourdatafilteringandgenerationpipeline, as BLEU (Papineni et al., 2002), ROUGE (Lin,
MMReviewcomprises240high-qualitysam- 2004),BERTScore(Zhangetal.,2020),andME-
plesacross17academicfieldsin4disciplines. TEOR (Banerjee and Lavie, 2005). In addition,
severalstudies(Robertson,2023;Zhouetal.,2024;
‚Ä¢ Wemeticulouslydesign13distincttasksen-
Gaoetal.,2025)haveadoptedtheLLM-as-a-judge
compassingatotalof6,724thoughtfullycu-
paradigm,leveragingcutting-edgelanguagemod-
rated questions, enabling multi-dimensional
els to assess the quality of review comments pro-
evaluation of model performance. These di-
duced by other LLMs. Given the absence of an
versetasksallowfortargetedassessmentand
establishedgoldstandardforthisevaluationtask,
facilitatetheidentificationofpotentiallimita-
recent research (Xu et al., 2024) has introduced
tionsinLLM-generatedpeerreviewcontent.
theGenerativeEstimatorforMutualInformation
(GEM) to quantify the degree of semantic over-
‚Ä¢ Weconductextensiveexperimentson18open-
lapbetweenLLM-generatedandhuman-authored
sourceand3closed-sourcemodelsusingthe
reviews. Nevertheless,existingevaluationmethod-
MMReview benchmark, offering some key
ologies are not grounded in a unified benchmark
findingsofLLM-basedautomatedreviewing.
ortaskframework,andtheyfallshortofproviding
Ourfindingsofferin-depthanalysisandvalu-
a comprehensive analysis of the underlying rea-
able guidance for the future development of
soningprocessesinvolvedinLLM-generatedpeer
LLM-assistedpeerreviewsystems.
review.
2 RelatedWorks
3 MMReviewBenchmark
2.1 LLMsforPaperReview
Inthissection,wefirstpresenttheoverallpipeline
LLMs have shown strong potential in analyzing
fordatacollectionandconstructionoftheMMRe-
complexscholarlytexts(LiuandShah,2023;Zhao
viewbenchmark,followedbyadetailedexposition
et al., 2024; Zhuang et al., 2025). Initial stud-
ofthetaskdesignmethodology.
iesindicatethatLLM-generatedreviewcomments
partially overlap with those of human reviewers, 3.1 OverallPipelineofMMReview
suggesting their potential contribution to peer re- Benchmark
view(Robertson,2023;Liangetal.,2023). How-
AsillustratedinFigure1,theconstructionofMM-
ever, further research reveals that even advanced
Reviewisdividedintothreestages: datacollection,
models like GPT-4o often fail to meet human ex-
dataprocessing,andtaskconstruction.
pectationsinreviewquality(Zhouetal.,2024). To
improvealignmentwithpeerreviewstandards,re- 3.1.1 DataCollection
searchers have built datasets from public review Duringthedatacollectionphase,wegatheredaca-
platformsandfine-tunedLLMs(Kangetal.,2018; demicpapersfrompubliclyaccessiblepeerreview
Yuanetal.,2021a;Shenetal.,2022;Dyckeetal., platforms or sources where reviewer comments
2023; Gao et al., 2024). Other methods involve were openly available. These papers not only
multi-turndialogue(Tanetal.,2024)ormulti-agent contain the full manuscript texts but also include
2
Data Collection Data Processing Task Construction
115
ùë´ Human ùë´ ùíÉ A R C E C J6 5 5 0 A R C E 6 C J 3 3 2 4 9 A R C E 3 C J 8 1 1 9 9 A R C E 2 C J 4 1 1 3 1
Data Filtering AI NS ES SS
Filtering
Review TaskGeneration
ùë´ Golden Answer Extraction
Small LLM ùíÉ Human
Verification
Attack654
ùë´ Preference96
ùë∏ùë®
Papers Reviews Decisions PDF Step782 Outcome768
Parsing Screen
Domain Classification Shot
Human Verification
Human Verification
34538 MMReview
ùë´ 13398 2880 1065 Text Figures Tables Benchmark
AI NS ES SS
Figure1: TheconstructionpipelineofMMReview. Theconstructionpipelineisdividedintothreestages: data
collection,dataprocessing,andtaskconstruction.
reviewer-writtenevaluationsandfinaldecisions(ac- Discipline ResearchField #Samples ACC:REJ
ceptorreject). Specifically, wecuratedatotalof MachineLearning 17 8:9
ComputerVision 20 12:8
51,881 papers with associated reviews submitted
NaturalLanguageProcessing 16 7:9
Artificial
between 2013 and 2024 to venues such as ICLR ReinforcementLearning 11 3:8
Intelligence
GraphNeuralNetworks 19 8:11
andNeurIPS(fromtheOpenReviewplatformand
SignalProcessing 17 8:9
NeurIPSProceedings,aswellasarticlesfromthe AIApplication 15 4:11
journal Nature Communications . For each col- Biology&Medicine 11 5:6
Physics 16 7:9
lectedpaper,weemployedDeepseek-V3andGPT- Natural Chemistry 13 5:8
Sciences
4o to automatically infer its academic discipline Environmental&EarthSciences 10 7:3
Mathematics&Statistics 13 5:8
andresearchdomain. Incaseswherethetwomod-
MaterialsScience 14 6:8
els produced inconsistent classifications, human Engineering ControlScience 12 5:7
verificationwasperformed. Ultimately,allpapers Sciences ElectronicInformation 6 4:2
EnergyScience 6 4:2
werecategorizedintofouroverarchingdisciplines:
Social
Society,Economics&Finance 24 11:13
ArtificialIntelligence,NaturalSciences,Engineer- Sciences
ing Sciences, and Social Sciences, resulting in a
Table 1: The distribution of papers across various re-
seeddatasetdenotedasD. Figure1illustratesthe
searchdomains.
distribution of papers across the four disciplines
withinD.
3.1.2 DataProcessing
et al., 2025) to generate summaries of each pa-
Duringthedataprocessingstage,wefirstfiltered perundertwodistinctinputconditions: oneusing
thecollectedseeddatasetD toobtainhigh-quality only the abstract and the other using the full text
samples while maintaining a relatively balanced ofthemanuscript. Agreaterdivergencebetween
distributionacrossdisciplinesandensuringacom- thetwogeneratedsummariesisinterpretedasevi-
parable number of accepted and rejected papers. dencethatthefulltextprovidessubstantiallymore
Toachievethis,wedesignedadual-pathjointdata information,thusindicatinghighersamplequality.
selectionmechanismthatsimultaneouslyfiltersfor Sampleswithasignificantinformationgainfrom
samplequalityandenforcesdistributionalbalance. thefulltextwereretainedinourtestbenchmark. To
Specifically, we utilized Qwen3-32B1 (Yang furtherensurethatthebenchmarkmaintainsabal-
anceddistributionacrossacademicdisciplinesand
1Qwen3-32Branksamongthetop10ontheOpenCom- anapproximatelyequalratioofacceptedtorejected
passleaderboard,offeringafavorabletrade-offbetweenper-
papers,wesupplementedthebenchmarkbymanu-
formanceandmodelsize,makingitefficientforlarge-scale
samplefiltering allyincorporatingtop-rankedpapersfromspecific
3
domainsinD basedonthequalityrankings. This The Summary task evaluates a model‚Äôs ability to
human-filteringprocedureguaranteesthatthecom- distillkeyinformationfromafullmanuscriptinto
positionofthebenchmarkdatasetalignswithour an accurate, concise summary. In this task, the
desireddistributionalproperties. modelispromptedtogenerateabriefsummaryin
During the data filtering phase, we obtained a itsownwords,avoidingabstractcopyingandsub-
total of 240 paper samples spanning 17 research jectivejudgment. Themodel-generatedsummary
domainsacross4majordisciplinestoconstructour isthencomparedtohuman-reviewer-writtensum-
evaluationbenchmark,denotedasD . Thestatisti- maries and evaluated for semantic similarity and
b
caldetailsofD ,includingthenumberofsamples informationcoverage,measuringthemodel‚Äôsholis-
b
per domain and the distribution of accepted ver- ticcomprehensionandrepresentationofacademic
susrejectedpapers,arepresentedinTable1. For content.
each discipline, we ensured a relatively balanced
number of samples across research fields while Strengths Evaluation (SE) and Weaknesses
approachingtheactualacceptance/rejectionratio Evaluation(WE) Summarizingandanalyzinga
throughacombinationofmodel-basedfilteringand manuscript‚Äôsstrengthsandweaknessesisacoreas-
manual curation. For each of these 240 samples, pectofpeerreview. TheStrengthsEvaluationand
we utilized PDF parsing tools to extract textual WeaknessesEvaluationtasksassessLLMs‚Äôability
content, figures, and tables from the manuscript toidentifyandarticulatethemeritsandlimitations
files,andconvertedeachpageofthePDFintocor- ofacademicpapers. Thesetaskstestwhethermod-
responding images. As a result, we constructed elscansynthesizetechnicalhighlightsandmethod-
three distinct modalities of input data: text-only, ological concerns noted by human reviewers, fo-
multimodal (text combined with extracted visual cusing on four dimensions: Quality, Clarity, Sig-
elements),andPDF-as-image. nificance,andOriginality. IntheStrengthsEvalu-
ationtask,modelsargueforacceptancebydetail-
3.1.3 TaskConstruction
ingmethodologicalrigor,experimentalrobustness,
During task construction, we developed 13 tasks structuralclarity,researchimpact,andnovelty,thus
groupedintofourthematiccategories,step-based, evaluating their capacity to extract technical con-
outcome-based, preference-based, and attack- tributions and assess scientific merit. In contrast,
based, reflecting the peer review workflow and theWeaknessesEvaluationtaskadoptsarejection-
challenges LLMs may face. Task distribution is orientedstance,testingcriticalreasoningandcon-
showninFigure1. Promptsweredesignedbased structive critique. Model outputs are compared
onreviewerguidelinesfrommajoracademiccon- withhumanreviewsbasedoncontentcoverageand
ferences. For each task, we used regular expres- semanticsimilarity.
sionsorGPT-4otoextractreferenceanswersfrom
humanreviewsinD ,formingthequestion‚Äìanswer SoundnessScoring(SS)andPresentationScor-
b
datasetD . FortheFakeStrengthEvaluationand ing (PS) The Soundness Scoring and Presenta-
QA
FakeWeaknessEvaluationtasks,GPT-4oandcus- tionScoringtasksevaluateLLMs‚Äôabilitytoquan-
tompromptsgeneratedantonymicrewrites,which titatively assess manuscript quality, focusing on
weremanuallyverifiedforsemanticaccuracy. This technical soundness and writing presentation. In
processfinalizedtheMMReviewbenchmark. The SoundnessScoring,themodelratesthereliability
detailedpromptsfortaskgenerationandeachtask oftechnicalclaims,experimentalrigor,andeviden-
canbefoundinAppendixD. tial support, emphasizing empirical and method-
ological validity. Presentation Scoring assesses
3.2 Step-basedTasks
linguisticclarityandlogicalorganization,reflect-
The Step-based theme comprises five tasks de- ingwritingqualityandinformationstructure. Both
signedtoprogressivelyevaluatetheperformance tasksrequireintegerscoresfrom1to4,denoting
ofLLMsinsimulatingthekeycomponentsofthe ‚Äúpoor"to‚Äúexcellent."Modelscoresarecompared
academicpeerreviewprocess. tohumanratingstoassessjudgmentconsistency.
Summary (S) Summarizing a paper is the ini-
3.3 Outcome-basedTasks
tialstepinpeerreviewandakeytestofamodel‚Äôs
abilitytoextractessentialcontent. Inaccuratesum- The Outcome-based tasks focus on assessing a
mariesmayimpairsubsequentreviewgeneration. model‚Äôs direct capability to generate peer review
4
outcomes,withthegoalofevaluatingitsalignment reviewsratherthanmerelyaveragingscores. This
withhumanreviewersinfinaldecision-making. taskmirrorstherealroleofanACandrigorously
evaluates the model‚Äôs capacity for synthesis and
ConditionalDecision(CD) TheConditionalDe-
decisionrationality. Modeloutputsarecompared
cision task assesses LLMs‚Äô ability to synthesize
tohuman-writtenmeta-reviewstoassessreliability
human-written reviews and generate an overall
andscientificjudgmentinhigh-levelpeerreview.
qualityscoreforapaper. Providedwithreviewer
comments detailing strengths, weaknesses, and 3.4 Preference-basedTask
evaluationsofmethodologyandresults,themodel
PairwiseRank(PR) Priorworkhasshownthat
assigns a numerical score from 1 to 10, reflect-
pairwisecomparisoneffectivelyevaluatesthealign-
ingascalefrom‚Äúfundamentallyflawedorlacking
mentbetweenLLM-generatedpreferencesandhu-
novelty‚Äùto‚Äúgroundbreakingcontribution,‚Äùaligned
manjudgments(Liuetal.,2024b). Sinceacademic
withacademicconferencestandards. Thetaskeval-
conferenceacceptancetiers,oral,spotlight,poster,
uates the model‚Äôs capacity to interpret sentiment,
reject,reflecthumanpreferencerankings,thePair-
weighting,andevaluativereasoninginthereviews
wiseRanktaskisdesignedtotestwhetherLLMs,
andtranslatethemintoacoherentquantitativejudg-
as reviewers, display preference patterns consis-
ment. Model scores are compared with human
tent with human evaluators. This task assesses
ratingstoassessalignmentandaccuracyinreview-
themodel‚Äôsrelativejudgmentabilitybypresenting
baseddecision-making.
pairsofpapersfromdifferentacceptancetiers: oral
DirectDecision(DD)andCoTDecision(CoD) (top 5%), spotlight (top 25%), poster, and reject.
TheDirectDecisionandCoT(Chain-of-Thought) Themodelcomparesandranksthepapers,simulat-
Decision tasks evaluate LLMs‚Äô ability to au- ingreal-worldpeerreviewselection. Alignmentis
tonomouslyassessacademicpaperquality,reflect- measuredbycomparingmodelpreferenceswithac-
ing two reviewing paradigms: streamlined judg- tualacceptancecategoriestodeterminerankingac-
mentandstep-by-stepreasoning(Weietal.,2022). curacy. Toreducepositionalbias(Shietal.,2025;
Thesetasksvaryininputformatandcognitivecom- Thakuretal.,2025),eachcomparisonisrepeated
plexity,enablingcontrolledcomparisonofmodel withreversedinputorder.
performance under different reasoning demands.
3.5 Attack-basedTasks
In the Direct Decision task, the model produces
anoverallscorewithoutguidance,simulatingare- Thistaskcategoryassessesmodels‚Äôrobustnessand
viewer‚Äôsholisticjudgmentfromasingleread. In discriminativeabilityinpeerreviewbyintroducing
contrast,theCoTDecisiontaskguidesthemodel adversarial inputs. Fabricated strengths or weak-
throughastructuredreasoningprocess: summariz- nesses,suchasinvertedprosandcons,areusedto
ingthepaper,analyzingstrengthsandweaknesses testthemodel‚Äôscapacitytodetectfactualinaccu-
acrossQuality,Clarity,Significance,andOriginal- racies. Misleading prompts further aim to divert
ity,assigningSoundnessandPresentationscores, the model from its original instructions. Model
and synthesizing an overall score. This mirrors outputsarecomparedtohumanjudgmentsorthe
a reviewer‚Äôs iterative, analytical evaluation. The model‚Äôsownnon-adversarialresponses,focusing
taskstestreasoningabilityandscoringtraceability. onitsabilitytopreserveevaluativeindependence
Model outputs are compared to human scores to andlogicalconsistencyunderinputperturbations.
assessconsistencyandrationality.
FakeStrengths(FS)andFakeWeaknesses(FW)
MetaDecision(MD) Beyondgeneratingindivid- LLMs have been widely observed to exhibit a
ual reviews and scores, a key aspect of academic "sycophantic"tendencyandapropensitytoalign
peerreviewistheAreaChair‚Äôs(AC)synthesisof withhumanjudgments(Liuetal.,2024a;Kumaran
reviewer feedback to make a final decision. To etal.,2025),whichmaycompromisetheirability
emulatethis, theMetaDecisiontaskrequiresthe toidentifyflawsandgeneratecriticalpeerreview
modeltoissueabinaryjudgment,AcceptorReject, comments. Additionally, the hallucination issue
basedonmultiplehumanreviews. Thepromptpro- inherent in LLMs may lead to erroneous assess-
vides structured guidance and evaluation criteria, ments of a paper‚Äôs strengths and weaknesses. To
prompting step-by-step reasoning. The model is investigatethesechallenges,weproposetheFake
instructedtoassessthequalityandconsistencyof StrengthandFakeWeaknesstasks,designedtoeval-
5
